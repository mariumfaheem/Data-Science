{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark Treasure.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMCFIDs+zsqw8nE5d+Cf3ml",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariumfaheem/Data-Science/blob/main/Spark_Treasure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEUtEafyvIMi"
      },
      "source": [
        "# **Spark Treasure**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShU8A-p8veP2"
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHtGNSx_vWMt"
      },
      "source": [
        "#Spark Sql"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU9_fX4RvVFV"
      },
      "source": [
        "import pandas as pd\n",
        "from pyspark import SparkContext,SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "# Creating a spark context class\n",
        "sc = SparkContext()\n",
        "\n",
        "# Creating a spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark DataFrames basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvwRg_9Wvium"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3zi-grMvi5-"
      },
      "source": [
        "# Read the file using `read_csv` function in pandas\n",
        "mtcars = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/mtcars.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s11tiprgvm9e"
      },
      "source": [
        "mtcars.rename( columns={'Unnamed: 0':'name'}, inplace=True )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bHIk9zdvr7V"
      },
      "source": [
        "sdf = spark.createDataFrame(mtcars) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtzfE1k2vsIF"
      },
      "source": [
        "sdf.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76EkdCiPvuak"
      },
      "source": [
        "sdf.createTempView(\"cars\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxLvZaPcvudV"
      },
      "source": [
        "#Showing the whole table\n",
        "spark.sql(\"SELECT * FROM cars\").show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOfnPYAivugn"
      },
      "source": [
        "# Basic filtering query to determine cars that have a high mileage and low cylinder count\n",
        "spark.sql(\"SELECT * FROM cars WHERE mpg > 20 AND  cyl <6 \").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTAJHgAKv4OX"
      },
      "source": [
        "# Aggregating data and grouping by cylinders\n",
        "spark.sql(\"SELECT count(*), cyl from cars GROUP BY cyl\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKH7Pd00v92K"
      },
      "source": [
        "Create a Pandas UDF to apply a columnar operation\n",
        "Apache Spark has become the de-facto standard in processing big data. To enable data scientists to leverage the value of big data, Spark added a Python API in version 0.7, with support for user-defined functions (UDF). These user-defined functions operate one-row-at-a-time, and thus suffer from high serialization and invocation overhead. As a result, many data pipelines define UDFs in Java and Scala and then invoke them from Python.\n",
        "\n",
        "Pandas UDFs built on top of Apache Arrow bring you the best of both worldsâ€”the ability to define low-overhead, high-performance UDFs entirely in Python. In this simple example, we will build a Scalar Pandas UDF to convert the wT column from imperial units (1000-lbs) to metric units (metric tons).\n",
        "\n",
        "In addition, UDFs can be registered and invoked in SQL out of the box by registering a regular python function using the @pandas_udf() decorator. We can then apply this UDF to our wt column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOl3vZw-v4Z9"
      },
      "source": [
        "# import the Pandas UDF function \n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRwEHcVzv4fE"
      },
      "source": [
        "@pandas_udf(\"float\")\n",
        "def convert_wt(s: pd.Series) -> pd.Series:\n",
        "    # The formula for converting from imperial to metric tons\n",
        "    return s * 0.45\n",
        "\n",
        "spark.udf.register(\"convert_weight\", convert_wt)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cpjvycqwDVi"
      },
      "source": [
        "spark.sql(\"SELECT *, wt AS weight_imperial, convert_weight(wt) as weight_metric FROM cars\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1zesly9wM2i"
      },
      "source": [
        "#Spark DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cPborR1wDYz"
      },
      "source": [
        "# Preview a few records\n",
        "mtcars.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wIyFWsowaM7"
      },
      "source": [
        "# We use the `createDataFrame` function to load the data into a spark dataframe\n",
        "sdf = spark.createDataFrame(mtcars) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBesGngbwaPb"
      },
      "source": [
        "# Let us look at the schema of the loaded spark dataframe\n",
        "sdf.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmq_tUv9waSC"
      },
      "source": [
        "sdf.select('mpg').show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MabxO1Ttwvsa"
      },
      "source": [
        " Filtering and Columnar operations\n",
        "Filtering and Column operations are important to select relevant data and apply useful transformations.\n",
        "\n",
        "We first filter to only retain rows with mpg > 18. We use the filter() function for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugHoF3-UwaVg"
      },
      "source": [
        "\n",
        "sdf.filter(sdf['mpg']<18).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfXZ_zUZw0wU"
      },
      "source": [
        "Operating on Columns\n",
        "\n",
        "Spark also provides a number of functions that can be directly applied to columns for data processing and aggregation. The example below shows the use of basic arithmetic functions to convert the weight values from lb to metric ton. We create a new column called wtTon that has the weight from the wt column converted to metric ton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgy7vhdvw1f_"
      },
      "source": [
        "sdf.withColumn('wtTon', sdf['wt'] * 0.45 ).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvLQpZ_Ew5m0"
      },
      "source": [
        "#Grouping and Aggregation\n",
        "Spark DataFrames support a number of commonly used functions to aggregate data after grouping. In this example we compute the average weight of cars by their cylinders as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPk0tLqXxnpo"
      },
      "source": [
        "sdf.groupby(['cyl'])\\\n",
        ".agg({\"wt\": \"AVG\"})\\\n",
        ".show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4JxBesPxnz5"
      },
      "source": [
        "car_counts = sdf.groupby(['cyl'])\\\n",
        ".agg({\"wt\": \"count\"})\\\n",
        ".sort(\"count(wt)\", ascending=False)\\\n",
        ".show(5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}